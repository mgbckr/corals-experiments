{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* run the Docker container as explained in the `README.md`\n",
    "* install and run Spark\n",
    "\n",
    "```bash\n",
    "# install java\n",
    "mkdir -p /usr/share/man/man1\n",
    "apt update\n",
    "apt install -y procps default-jre\n",
    "\n",
    "# install spark libraries\n",
    "conda install -c conda-forge pyspark\n",
    "pip install joblibspark\n",
    "\n",
    "# download spark\n",
    "wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
    "tar -xf spark-3.3.0-bin-hadoop3.tgz\n",
    "cd spark-3.3.0-bin-hadoop3\n",
    "\n",
    "# start master\n",
    "./sbin/start-master.sh -h $HOSTNAME\n",
    "\n",
    "# start worker\n",
    "./sbin/start-worker.sh $HOSTNAME:7077\n",
    "```\n",
    "\n",
    "* you can then either run Jupyter within the container or execute the follwing lines in an `ipython` environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 77.9 ms, sys: 12.3 ms, total: 90.2 ms\n",
      "Wall time: 96.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# make sure we are not oversubscribing CPUs\n",
    "# by adjusting `n_threads`, `corals.cor_matrix` can be parallelized\n",
    "from corals.threads import set_threads_for_external_libraries\n",
    "set_threads_for_external_libraries(n_threads=1)\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import corals\n",
    "\n",
    "# create random data\n",
    "n_features = 16000\n",
    "n_samples = 32\n",
    "X = np.random.random((n_samples, n_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anes-nalab2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblibspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblibspark'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from joblibspark import register_spark\n",
    "register_spark() # register spark backend\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(f\"spark://{hostname}:7077\") \\\n",
    "    .appName(\"Corals\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CorALS top-k in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corals.correlation.topk import cor_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_jobs = 64  # change this an observe runtime differences\n",
    "with parallel_backend('spark'):\n",
    "    result = cor_topk(\n",
    "        X, \n",
    "        k=0.001,\n",
    "        approximation_factor=10,\n",
    "        correlation_type=\"spearman\", \n",
    "        n_batches=n_jobs,\n",
    "        n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark correlation function\n",
    "Source: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.stat.Correlation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseMatrix, Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "n_partitions = 12\n",
    "Xspark = [[Vectors.dense(row)] for row in X]\n",
    "dataset = spark.createDataFrame(Xspark, ['features']).repartition(n_partitions)\n",
    "dataset.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# * may throw errors (possibly heap issues; needs Spark configuration?)\n",
    "# * slow\n",
    "# * results in full correlation matrix in memory\n",
    "cor = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.5 ms, sys: 0 ns, total: 48.5 ms\n",
      "Wall time: 47.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from corals.correlation.utils import derive_k, argtopk\n",
    "\n",
    "# derive a valid top-k\n",
    "k = derive_k(X, Y=None, k=0.001)\n",
    "\n",
    "# sort correlations\n",
    "cor = cor.flatten()\n",
    "\n",
    "topk_idx_flat = argtopk(-np.abs(cor), k=k, threshold=None)   \n",
    "\n",
    "# derive topk correlation and index \n",
    "topk_cor = cor[topk_idx_flat]\n",
    "topk_idx = np.unravel_index(topk_idx_flat, (X.shape[1], X.shape[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nalab-fastcor-test",
   "language": "python",
   "name": "nalab-fastcor-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec96ce1ba632f2007942df008dbcd1ea1e3ee2feebc076cf27f86e8586243e05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
