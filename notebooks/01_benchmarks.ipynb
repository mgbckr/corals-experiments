{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# general\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import warnings \n",
    "import joblib\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# ml / stats\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# init matplotlib defaults\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m rpy2.situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_dir = pathlib.Path(\"../_out/benchmark\")\n",
    "out_dir = pathlib.Path(\"../_out/figures\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "method_name = \"CorALS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load runtime / memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_benchmark(file, benchmark_dir=\"../_out/benchmark\"):\n",
    "    \n",
    "    benchmark_dir = pathlib.Path(benchmark_dir)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    def visit(name, data):\n",
    "        if isinstance(data, h5py.Group):\n",
    "            if \"memory\" in data:\n",
    "                memory = data[\"memory\"][:]\n",
    "                runtime = data[\"runtime\"][:]\n",
    "                if \"memory_backend\" in data.attrs:\n",
    "                    memory_backend = data.attrs[\"memory_backend\"]\n",
    "                else:\n",
    "                    memory_backend = \"default\"\n",
    "                if \"timestamp\" in data.attrs:\n",
    "                    timestamp = datetime.datetime.fromtimestamp(data.attrs[\"timestamp\"])\n",
    "                else:\n",
    "                    timestamp = None\n",
    "                results.extend([(name, i, m, r, memory_backend, timestamp) for i, (m, r) in enumerate(zip(memory, runtime))])\n",
    "            else:\n",
    "                print(\"Skipping: \", name)\n",
    "                \n",
    "    with h5py.File(benchmark_dir / file, \"r\") as f:\n",
    "        f.visititems(visit)\n",
    "\n",
    "    results = pd.DataFrame(results, columns=[\"algorithm\", \"repetition\", \"memory\", \"runtime\", \"memory_backend\", \"timestamp\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_benchmark_experiments(benchmark_dir=\"../_out/benchmark\"):\n",
    "\n",
    "    benchmark_dir = pathlib.Path(benchmark_dir)\n",
    "    \n",
    "    benchmark_experiments = []\n",
    "\n",
    "    for f in sorted(benchmark_dir.glob(\"benchmark_*\")):\n",
    "        \n",
    "        print(f.name)\n",
    "\n",
    "        results = load_benchmark(f, \"\")\n",
    "\n",
    "        results[\"language\"] = re.search(\"lang-(.*?)___\", f.name).group(1)\n",
    "        results[\"prefix\"] = re.search(\"prefix-(.*?)___\", f.name).group(1)\n",
    "        \n",
    "        context = re.search(\"context-(.*?)___\", f.name).group(1)\n",
    "        context_param = None\n",
    "        if context.startswith(\"topk\"):\n",
    "            context_param = float(context.split(\"-\")[1].replace(\"percent\", \"\"))\n",
    "            context = context.split(\"-\")[0]\n",
    "        results[\"context\"] = context    \n",
    "        results[\"context_param\"] = context_param\n",
    "        \n",
    "        # data\n",
    "        results[\"data\"] = re.search(\"data-(.*?)___\", f.name).group(1)\n",
    "        \n",
    "        # repeat\n",
    "        results[\"n_repeat\"] = int(re.search(\"repeat-(.*?)(.h5|___)\", f.name).group(1))\n",
    "\n",
    "        # filename\n",
    "        results[\"file\"] = f.name\n",
    "#         display(results)\n",
    "        \n",
    "        benchmark_experiments.append(results)\n",
    "\n",
    "    benchmark_experiments = pd.concat(benchmark_experiments)\n",
    "\n",
    "    # dropping first run (0) to account for compiler optimization\n",
    "    # this is a bit questionable depending on application scenario\n",
    "    benchmark_experiments = benchmark_experiments[benchmark_experiments.n_repeat >= 1] \n",
    "    return benchmark_experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_accuracy(file, benchmark_dir=\"../_out/benchmark\"):\n",
    "    \n",
    "    benchmark_dir = pathlib.Path(benchmark_dir)\n",
    "\n",
    "    results = collections.OrderedDict()\n",
    "\n",
    "    def visit(name, data):\n",
    "        if isinstance(data, h5py.Dataset) and \"metrics/\" in name:\n",
    "            results[name.split(\"/\")[1]] = data[...]\n",
    "\n",
    "    with h5py.File(benchmark_dir / file, \"r\") as f:\n",
    "        approximation_factors = f[\"approximation_factors\"][...]\n",
    "        f.visititems(visit)\n",
    "        \n",
    "    df = pd.DataFrame(results, index=approximation_factors)\n",
    "    df.index.name = \"approximation_factor\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_accuracy_experiments(benchmark_dir=\"../_out/benchmark\"):\n",
    "\n",
    "    benchmark_dir = pathlib.Path(benchmark_dir)\n",
    "    \n",
    "    exps = []\n",
    "\n",
    "    for f in sorted(benchmark_dir.glob(\"ac*curacy_*\")):\n",
    "        print(f.name)\n",
    "\n",
    "        results = load_accuracy(f, \"\")\n",
    "        results[\"data\"] = re.search(\"data.(.*?)___\", f.name).group(1)\n",
    "        results[\"context\"] = re.search(\"(topk(diff)?)\", f.name).group(1)\n",
    "#         results[\"topk\"] = \"moob\"\n",
    "#         results[\"topk\"] = float(re.search(\"topk(diff)?-(.*?)percent\", f.name).group(1))\n",
    "#         results[\"topkdiff\"] = float(re.search(\"topkdiff-(.*?)percent\", f.name).group(1))\n",
    "        results[\"context_param\"] = float(re.search(\"topk(diff)?-(.*?)percent\", f.name).group(2))\n",
    "    \n",
    "        if \"method\" in f.name:\n",
    "            results[\"method\"] = re.search(\"method-(.*)[\\._]\", f.name).group(1)\n",
    "        else:\n",
    "            results[\"method\"] = 'unknown'\n",
    "            \n",
    "        if \"spearman\" in f.name:\n",
    "            results[\"spearman\"] = re.search(\"spearman-(.*)[\\._]\", f.name).group(1)\n",
    "        else:\n",
    "            results[\"spearman\"] = 'unknown'\n",
    "\n",
    "        exps.append(results)\n",
    "\n",
    "    exps = pd.concat(exps)\n",
    "\n",
    "    # dropping first run (0) to account for compiler optimization\n",
    "    # this is a bit questionable depending on application scenario\n",
    "    return exps.reset_index().set_index([\"data\", \"context\", \"context_param\", \"method\", \"spearman\", \"approximation_factor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_timespan(timespan_in_seconds):\n",
    "    if np.isnan(timespan_in_seconds):\n",
    "        return \"-\"\n",
    "    \n",
    "    hours = timespan_in_seconds // 60**2\n",
    "    minutes = timespan_in_seconds // 60 - hours * 60\n",
    "    seconds = timespan_in_seconds - minutes * 60 - hours * 60**2\n",
    "    if minutes > 0:\n",
    "        formatted = f'{seconds:04.1f}'\n",
    "    else:\n",
    "        formatted = f'{seconds:.1f}'\n",
    "    if minutes > 0:\n",
    "        if hours > 0:\n",
    "            formatted = f'{minutes:02.0f}:' + formatted\n",
    "        else:\n",
    "            formatted = f'{minutes:.0f}:' + formatted\n",
    "    if hours > 0:\n",
    "        formatted = f'{hours:.0f}:' + formatted\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_memory(memory_in_mb):\n",
    "    if np.isnan(memory_in_mb):\n",
    "        return \"-\"\n",
    "    return f\"{memory_in_mb / 1024:.01f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets_map = collections.OrderedDict([\n",
    "    (\"preeclampsia_postprocessed_nonegatives_dropduplicates\", \"Preeclampsia\"),\n",
    "    (\"pregnancy_postprocessed_nonegatives_dropduplicates\", \"Pregnancy\"),\n",
    "    (\"cancer_postprocessed_nonegatives_dropduplicates_sample-0.25\", \"Cancer (0.25)\"),\n",
    "    (\"cancer_postprocessed_nonegatives_dropduplicates_sample-0.50\", \"Cancer (0.50)\"),\n",
    "    (\"cancer_postprocessed_nonegatives_dropduplicates_sample-1.00\", \"Cancer (1.00)\"),\n",
    "    (\"singlecell_postprocessed\", \"Single Cell\"),\n",
    "    (\"singlecell_large_postprocessed\", \"Single Cell 2\"),\n",
    "    (\"large_synthetic_mn_m-500_n-200000_postprocessed\", \"Sim\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_benchmarks(\n",
    "        algorithms=None, global_select=True, statistic=\"runtime\", statistic_formatter=\"auto\", column_regex=None, stats=\"default\"):\n",
    "    \n",
    "    stats = []\n",
    "    for algorithm_name, algorithm_select in algorithms:\n",
    "        # select data\n",
    "        df = benchmark_experiments.loc[global_select & algorithm_select].copy()\n",
    "        if df.shape[0] == 0:\n",
    "            df = pd.DataFrame(dict(data=[\"dummy\"], algorithm=algorithm_name, statistic=[-1]))\n",
    "        df[\"algorithm\"] = algorithm_name\n",
    "        stats.append(df) \n",
    "    stats = pd.concat(stats)\n",
    "    \n",
    "    # calculate stats\n",
    "    stats = stats[[\"data\", \"algorithm\", statistic]].groupby([\"algorithm\", \"data\"]).agg([\"median\", \"mean\", \"std\", \"count\"])\n",
    "    \n",
    "    # pull up algorithms into columns\n",
    "    stats = stats.unstack(level=0).reorder_levels([0,2,1], axis=1)\n",
    "    \n",
    "    # sort algorithms\n",
    "    columns = [c for n, _ in algorithms for c in stats.columns if c[1] == n]\n",
    "    stats = stats[columns]\n",
    "    \n",
    "    # rename datasets\n",
    "    stats.rename(datasets_map, axis=0, level=\"data\", inplace=True)\n",
    "    \n",
    "    # filter unwanted stats\n",
    "    if column_regex is not None:\n",
    "        stats = stats.filter(regex=column_regex)\n",
    "      \n",
    "    # sort datasets (and drop unwanted ones)\n",
    "    stats = stats.loc[[d for d in datasets_map.values() if d in stats.index],:]  \n",
    "    \n",
    "    # set formatter\n",
    "    if statistic_formatter == \"auto\":\n",
    "        if statistic == \"runtime\":\n",
    "            statistic_formatter = format_timespan\n",
    "        elif statistic == \"memory\":\n",
    "            statistic_formatter = format_memory\n",
    "        else:\n",
    "            raise ValueEror(f\"No known formatter fo statistic: {statistic}\")\n",
    "    \n",
    "    # format\n",
    "    if statistic_formatter is not None:\n",
    "        \n",
    "        formatted = stats.style\\\n",
    "            .format(statistic_formatter, subset=stats.filter(regex=\"median|mean|std\").columns)\n",
    "        \n",
    "        formatters = \\\n",
    "            [statistic_formatter if re.match(\"median|mean|std\", c[2]) else None for c in stats.columns]  \n",
    "        \n",
    "        latex = stats.to_latex(formatters=formatters, na_rep=\"-\")\n",
    "    else:\n",
    "        formatted = None\n",
    "        latex = None\n",
    "    \n",
    "    # done\n",
    "    return stats, formatted, latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load experiments and prepare visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_experiments = load_benchmark_experiments()\n",
    "benchmark_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "benchmark_experiments[\n",
    "    (benchmark_experiments.prefix == \"full_default\") \n",
    "    & (benchmark_experiments.context == \"fast\") \n",
    "    & (benchmark_experiments.language == \"julia\") \n",
    "    & (benchmark_experiments.algorithm == \"cor_cor\")\n",
    "    & (benchmark_experiments.data.str.contains(\"0.5\"))].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # experiments overview\n",
    "# experiment_identifiers = [\"language\", \"prefix\", \"context\", \"context_param\", \"data\"]\n",
    "# benchmark_experiments[experiment_identifiers].drop_duplicates().sort_values(experiment_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_experiments = load_accuracy_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "styles = [\"-\", \"--\", \":\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "patchList = []\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[0], linestyle=styles[0], \n",
    "    label=\"Python default\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[0], linestyle=styles[1], \n",
    "    label=f\"Python {method_name}\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[1], linestyle=styles[0], \n",
    "    label=\"Julia default\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[1], linestyle=styles[1], \n",
    "    label=f\"Julia {method_name}\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[2], linestyle=styles[0], \n",
    "    label=\"R default\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[2], linestyle=styles[1], \n",
    "    label=f\"R {method_name}\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "\n",
    "fig, axes = plt.subplots(1,1, dpi=150, figsize=(2,2))\n",
    "ax = axes\n",
    "ax.legend(handles=patchList, loc=\"upper left\", frameon=False)\n",
    "ax.axis(\"off\")\n",
    "figures_path = pathlib.Path('../_out/figures')\n",
    "figures_path.mkdir(parents=True, exist_ok=True)\n",
    "fig.savefig(figures_path / 'legend.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_stats = [\"prefix\", \"language\", \"algorithm\", \"data\", \"memory\", \"runtime\", \"context_param\", \"memory_backend\"]\n",
    "agg_stats = [\"median\", \"mean\", \"std\", \"size\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import coralsarticle.data.utils\n",
    "data_path = pathlib.Path(\"../data/benchmark/\")\n",
    "data = collections.OrderedDict()\n",
    "for p in data_path.glob(\"*\"):\n",
    "    if re.match(\"(pree|preg|canc|sing|large_synthetic)\", p.name): \n",
    "        df = coralsarticle.data.utils.load_h5(p)\n",
    "        name = re.search(\"(.*).h5\", p.name).group(1)\n",
    "        data[name] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_data(d, name):\n",
    "    df_data = pd.DataFrame({\n",
    "        \"Dataset\": [name],\n",
    "        \"Features\": [d.shape[1]],\n",
    "        \"Samples\": [d.shape[0]],\n",
    "        \"Feature/Sample Ratio\": [int(np.round(d.shape[1] / d.shape[0]))],\n",
    "    })\n",
    "    return df_data\n",
    "\n",
    "data_df = pd.concat([\n",
    "    format_data(df, datasets_map[n]) for n, df in data.items()\n",
    "])\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = data_df.Features\n",
    "y = x ** 2\n",
    "\n",
    "xx = np.linspace(min(x),600000)\n",
    "yy = xx**2\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.plot(xx, yy)\n",
    "\n",
    "\n",
    "ax.scatter(x, y, s=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean-up\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# everything with non-synthetic data\n",
    "select = \\\n",
    "    ~(benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \\\n",
    "    & (benchmark_experiments[\"context\"] == \"fast\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    r = benchmark_experiments[select][columns_stats]\\\n",
    "        .groupby([\"data\", \"language\", \"algorithm\", \"prefix\"])\\\n",
    "        .agg(agg_stats) \\\n",
    "        .sort_index()\n",
    "    display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select benchmark experiments\n",
    "b = benchmark_experiments\n",
    "algorithms_default = [  \n",
    "    (f\"{method_name}\",            (b.language == \"python\") & (b.algorithm == \"cor_matrix_symmetrical_nocopy\")),\n",
    "    (f\"{method_name} (parallel)\", (b.language == \"python\") & (b.algorithm == \"cor_matrix_symmetrical_nthreads-64\")),\n",
    "    (\"R\",             (b.language == \"r\")      & (b.algorithm == \"cor_cor\")),\n",
    "    (\"Julia\",         (b.language == \"julia\")  & (b.algorithm == \"cor_cor\")),\n",
    "    (\"Python\",        (b.language == \"python\") & (b.algorithm == \"cor_corrcoef\")),\n",
    "    (\"WGCNA\",                         (b.language == \"r\")      & (b.algorithm == \"cor_wgcna\")),\n",
    "    (\"coop\",                          (b.language == \"r\")      & (b.algorithm == \"cor_coop\")),\n",
    "    (\"Rfast\",                     (b.language == \"r\")      & (b.algorithm == \"cor_rfast\")),\n",
    "    (\"HiClimR\",                   (b.language == \"r\")      & (b.algorithm == \"cor_hiclimr\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# general filter\n",
    "global_select = \\\n",
    "    ~(benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \\\n",
    "    & (benchmark_experiments[\"prefix\"] == \"full_default\") \\\n",
    "    & (benchmark_experiments[\"context\"] == \"fast\") \\\n",
    "#     & (benchmark_experiments[\"language\"] == \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# long stats\n",
    "df, f, l = format_benchmarks(algorithms_default, global_select, statistic=\"runtime\", column_regex=\".*\")\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# long stats\n",
    "df, f, l = format_benchmarks(algorithms_default, global_select, statistic=\"memory\", column_regex=\".*\")\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stats for paper (runtime)\n",
    "df, f, l = format_benchmarks(algorithms_default, global_select, statistic=\"runtime\", column_regex=\"median\")\n",
    "display(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for paper (runtime)\n",
    "df, f, l = format_benchmarks(algorithms_default, global_select, statistic=\"memory\", column_regex=\"median\")\n",
    "display(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([16897, 32211, 64813, 129626, 200000, 259252])\n",
    "y = np.array([ 2.38289074,   8.038218,  31.60920241, 125.55584884])\n",
    "import sklearn.linear_model\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(np.log(x[:-2].reshape(-1,1)), np.log(y))\n",
    "\n",
    "yp = np.exp(lr.predict(np.log(x).reshape(-1,1)))\n",
    "\n",
    "\n",
    "xx = np.linspace(min(x),200000)\n",
    "yy = np.exp(lr.predict(np.log(xx).reshape(-1,1)))\n",
    "plt.scatter(xx, yy, s=10)\n",
    "\n",
    "plt.scatter(x[:-2], y, s=100)\n",
    "plt.scatter(x, yp, marker=\"x\", c=\"black\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xticks(x, x, rotation=320)\n",
    "plt.yticks(yp, [f\"{yy:.02f} Gb\" for yy in yp])\n",
    "plt.xlabel(\"number of features\")\n",
    "plt.ylabel(\"memory\")\n",
    "\n",
    "xx = np.array([16000, 32000, 64000, 128000, 250000])\n",
    "yy = np.exp(lr.predict(np.log(xx).reshape(-1,1)))\n",
    "for xxx, yyy in zip(xx, yy):\n",
    "    print(f\"features: {xxx:10d} -> memory: {yyy:10.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# everything with non-synthetic data\n",
    "\n",
    "select = \\\n",
    "        ~(benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \\\n",
    "        & (benchmark_experiments[\"context\"] == \"topk\")\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    r = benchmark_experiments[select][columns_stats]\\\n",
    "        .groupby([\"data\", \"language\", \"context_param\", \"algorithm\", \"prefix\", \"memory_backend\"])\\\n",
    "        .agg(agg_stats)\\\n",
    "        .sort_index()\n",
    "    display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = benchmark_experiments\n",
    "algorithms_default = [  \n",
    "    (\"R\",             (b.language == \"r\")      & (b.algorithm == \"topk_matrix\")),\n",
    "    (\"Julia\",         (b.language == \"julia\")  & (b.algorithm == \"topk_matrix\")),\n",
    "    (\"Python\",        (b.language == \"python\") & (b.algorithm == \"topk_matrix\")),\n",
    "    (f\"{method_name}\",            (b.language == \"python\") & (b.algorithm == \"topk_balltree_combined_tree\")),\n",
    "]\n",
    "algorithms_runtime = [\n",
    "    (f\"{method_name} (parallel)\", (b.language == \"python\") & (b.algorithm == \"topk_balltree_combined_tree_optimized_parallel_64\") & (b.memory_backend == \"psutil_uss\")),\n",
    "]\n",
    "algorithms_memory = [\n",
    "    (f\"{method_name} (parallel)\", (b.language == \"python\") & (b.algorithm == \"topk_balltree_combined_tree_optimized_parallel_64\") & (b.memory_backend == \"psutil_uss\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_select = \\\n",
    "    ~(benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \\\n",
    "    & (benchmark_experiments[\"context\"] == \"topk\") \\\n",
    "    & (benchmark_experiments[\"context_param\"] == 0.1) \\\n",
    "#     & (benchmark_experiments[\"language\"] == \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, f, l = format_benchmarks(algorithms_default + algorithms_runtime, global_select, statistic=\"runtime\", column_regex=\".*\")\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, f, l = format_benchmarks(algorithms_default + algorithms_runtime, global_select, statistic=\"runtime\", column_regex=\"median\")\n",
    "display(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, f, l = format_benchmarks(algorithms_default + algorithms_memory, global_select, statistic=\"memory\", column_regex=\"median\")\n",
    "display(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.array([16897, 32211, 64813, 129626, 200000, 259252, 600000])\n",
    "y = np.array([ 7.5, 27.3, 157.6])\n",
    "# y = np.array([ 6.4, 23.3, 93.8])\n",
    "# y = np.array([ 6.8, 23.7, 94.3])\n",
    "\n",
    "import sklearn.linear_model\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(np.log(x[:len(y)].reshape(-1,1)), np.log(y))\n",
    "\n",
    "yp = np.exp(lr.predict(np.log(x).reshape(-1,1)))\n",
    "\n",
    "\n",
    "xx = np.linspace(min(x),max(x))\n",
    "yy = np.exp(lr.predict(np.log(xx).reshape(-1,1)))\n",
    "plt.scatter(xx, yy, s=10)\n",
    "\n",
    "plt.scatter(x[:len(y)], y, s=100)\n",
    "plt.scatter(x, yp, marker=\"x\", c=\"black\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xticks(x, x, rotation=320)\n",
    "plt.yticks(yp, [f\"{yy:.02f} Gb\" for yy in yp])\n",
    "plt.xlabel(\"number of features\")\n",
    "plt.ylabel(\"memory\")\n",
    "\n",
    "xx = np.array([16000, 32000, 64000, 128000, 200000, 250000])\n",
    "yy = np.exp(lr.predict(np.log(xx).reshape(-1,1)))\n",
    "for xxx, yyy in zip(xx, yy):\n",
    "    print(f\"features: {xxx:10d} -> memory: {yyy:10.02f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Euclidean distance vs. correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(cor):\n",
    "    return np.sqrt(2 - 2*cor)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(2,2), dpi=300)\n",
    "c = np.linspace(-1,1,100)\n",
    "ax.plot(c, f(c), linewidth=2)\n",
    "ax.set(\n",
    "    xlabel=\"correlation\",\n",
    "    ylabel=\"Euclidean distance\")\n",
    "ax.axhline(np.sqrt(2), color=\"grey\", linestyle=\"--\")\n",
    "ax.axvline(0, color=\"grey\", linestyle=\"--\")\n",
    "\n",
    "fig.savefig(out_dir / \"dist-vs-cor.pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = benchmark_experiments\n",
    "algorithms = [  \n",
    "    (\"twice\",        (b.language == \"python\") & (b.algorithm == \"topk_balltree_twice\")),\n",
    "    (\"no-dual\",      (b.language == \"python\") & (b.algorithm == \"topk_balltree_combined_tree_no-dual\")),\n",
    "    (\"tree\",         (b.language == \"python\") & (b.algorithm == \"topk_balltree_combined_tree\")),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_select = \\\n",
    "    ~(benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \\\n",
    "    & (benchmark_experiments[\"context\"] == \"topk\") \\\n",
    "    & (benchmark_experiments[\"context_param\"] == 0.1) \\\n",
    "#     & (benchmark_experiments[\"language\"] == \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, f, l = format_benchmarks(algorithms, global_select, statistic=\"runtime\", column_regex=\"median\")\n",
    "display(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, f, l = format_benchmarks(algorithms, global_select, statistic=\"memory\", column_regex=\"median\")\n",
    "display(f)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full matrix - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark_experiments[\n",
    "    (benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \n",
    "    & (benchmark_experiments[\"context\"] == \"fast\") \n",
    "#     & (benchmark_experiments[\"algorithm\"].isin(algorithms)) \n",
    "].copy()\n",
    "results.loc[:, \"m\"] = results[\"data\"].str.extract(\".*m-(.*?)_.*\").values.flatten().astype(int)\n",
    "results.loc[:, \"n\"] = results[\"data\"].str.extract(\".*n-(.*?)_\").values.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.algorithm.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_python_final = [\n",
    "    \"cor_corrcoef\",\n",
    "#     \"cor_matrix_symmetrical\", \n",
    "    \"cor_matrix_symmetrical_nocopy\"\n",
    "]\n",
    "\n",
    "algorithms_julia_final = [\n",
    "    \"cor_cor\", \n",
    "#     \"cor_symmetrical\", \n",
    "    \"cor_symmetrical_nocopy2\"\n",
    "]\n",
    "\n",
    "algorithms_r_final = [\n",
    "    \"cor_cor\", \n",
    "#     \"cor_symmetrical\", \n",
    "    \"cor_symmetrical_nocopy\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "# fig, axes = plt.subplots(1,1, figsize=(10,10), dpi=150)\n",
    "ax = axes\n",
    "\n",
    "ax.axvline(20000, linestyle=\"-\", color=\"lightgrey\")\n",
    "\n",
    "color = colors[0]\n",
    "styles = [\"-\", \"--\", \":\", \"dashdot\", \"dashdot\",\"dashdot\",]\n",
    "for i, a in enumerate(algorithms_python_final):\n",
    "    select = results[(results[\"language\"] == \"python\") & (results[\"algorithm\"] == a) & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "    ax.plot(select.index, select[\"runtime\"], color=color, linestyle=styles[i], label=a, marker=\"+\")\n",
    "\n",
    "color = colors[1]\n",
    "styles = [\"-\", \"--\", \":\", \"dashdot\", \"dashdot\",\"dashdot\",]\n",
    "for i, a in enumerate(algorithms_julia_final):\n",
    "    print(a)\n",
    "    select = results[(results[\"language\"] == \"julia\") & (results[\"algorithm\"] == a) & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "    ax.plot(select.index, select[\"runtime\"], color=color, linestyle=styles[i], label=a, marker=\"+\")\n",
    "\n",
    "color = colors[2]\n",
    "styles = [\"-\", \"--\", \":\", \"dashdot\", \"dashdot\",\"dashdot\",]\n",
    "for i, a in enumerate(algorithms_r_final):\n",
    "    print(a)\n",
    "    select = results[(results[\"language\"] == \"r\") & (results[\"algorithm\"] == a) & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "    ax.plot(select.index, select[\"runtime\"], color=color, linestyle=styles[i], label=a, marker=\"+\")\n",
    "    \n",
    "    \n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "# ax.set_xlim([0,5000])\n",
    "# ax.set_ylim([0,0.1])\n",
    "# ax.set_xlim([10000,40000])\n",
    "# ax.set_ylim([1,10])\n",
    "# ax.set_xlim([5000,40000])\n",
    "# ax.set_ylim([0.1,10])\n",
    "\n",
    "# ax.set_xlim([2000,10000])\n",
    "ax.set_ylim([0.01,10])\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.set(xlabel=\"features\", ylabel=\"seconds\")\n",
    "\n",
    "ax.set_xticks([1000,3000,10000,30000])\n",
    "ax.set_xticklabels([1000,3000,10000,30000])\n",
    "\n",
    "ax.set_yticks([0.1, 1, 10])\n",
    "ax.set_yticklabels([0.1,1,10])\n",
    "\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# ax.set_xticks(select.index.values[:-1])\n",
    "# ax.set_xticklabels([str(int(x)) for x in select.index.values / 100][:-1])\n",
    "\n",
    "fig.savefig(out_dir / \"fast_runtime_features.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_python_final = [\n",
    "    \"cor_corrcoef\",\n",
    "#     \"cor_matrix_symmetrical\", \n",
    "    \"cor_matrix_symmetrical_nocopy\"\n",
    "]\n",
    "\n",
    "algorithms_julia_final = [\n",
    "    \"cor_cor\", \n",
    "#     \"cor_symmetrical\", \n",
    "    \"cor_symmetrical_nocopy2\"\n",
    "]\n",
    "\n",
    "algorithms_r_final = [\n",
    "    \"cor_cor\", \n",
    "#     \"cor_symmetrical\", \n",
    "    \"cor_symmetrical_nocopy\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "# fig, axes = plt.subplots(1,1, figsize=(10,10), dpi=150)\n",
    "ax = axes\n",
    "\n",
    "ax.axvline(50, linestyle=\"-\", color=\"lightgrey\")\n",
    "\n",
    "color = colors[0]\n",
    "for i, a in enumerate(algorithms_python_final):\n",
    "    select = results[(results[\"language\"] == \"python\") & (results[\"algorithm\"] == a) & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "    ax.plot(select.index, select[\"runtime\"], color=color, linestyle=styles[i], label=a, marker=\"+\")\n",
    "\n",
    "color = colors[1]\n",
    "styles = [\"-\", \"--\", \":\"]\n",
    "for i, a in enumerate(algorithms_julia_final):\n",
    "    print(a)\n",
    "    select = results[(results[\"language\"] == \"julia\") & (results[\"algorithm\"] == a) & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "    ax.plot(select.index, select[\"runtime\"], color=color, linestyle=styles[i], label=a, marker=\"+\")\n",
    "\n",
    "color = colors[2]\n",
    "styles = [\"-\", \"--\", \":\", \"dashdot\", (0, (5, 1)),(0, (3, 1, 1, 1, 1, 1))]\n",
    "for i, a in enumerate(algorithms_r_final):\n",
    "    print(a)\n",
    "    select = results[(results[\"language\"] == \"r\") & (results[\"algorithm\"] == a) & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "    ax.plot(select.index, select[\"runtime\"], color=color, linestyle=styles[i], label=a, marker=\"+\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "# ax.set_xlim([0,5000])\n",
    "# ax.set_ylim([0,0.1])\n",
    "# ax.set_xlim([10000,40000])\n",
    "# ax.set_ylim([1,10])\n",
    "# ax.set_xlim([5000,40000])\n",
    "# ax.set_ylim([0.1,10])\n",
    "\n",
    "ax.set_xlim([10,500])\n",
    "ax.set_ylim([0,10])\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.set(xlabel=\"samples\", ylabel=\"seconds\")\n",
    "\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "ax.set_xticks([10,25, 50,100, 200,500])\n",
    "ax.set_xticklabels([10,25,50, 100, 200,500])\n",
    "\n",
    "ax.set_yticks([1,2,5,10])\n",
    "ax.set_yticklabels([1,2,5,10])\n",
    "\n",
    "# ax.set_yticks([0.1, 1, 10, 50])\n",
    "# ax.set_yticklabels([0.1,1,10, 50])\n",
    "\n",
    "fig.savefig(out_dir / \"fast_runtime_samples.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-k - Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_python = [\"topk_matrix\", \"topk_balltree_combined_tree\"]\n",
    "algorithms_julia = [\"topk_matrix\", \"topk_balltree_nn_combined_tree\"]\n",
    "algorithms_r = [\"topk_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmark_experiments[\n",
    "    (benchmark_experiments[\"data\"].str.startswith(\"synthetic\")) \n",
    "    & (benchmark_experiments[\"context\"] == \"topk\") \n",
    "    & (benchmark_experiments[\"context_param\"] == 0.1)\n",
    "    & (benchmark_experiments[\"algorithm\"].isin(algorithms_python + algorithms_julia)) \n",
    "].copy()\n",
    "results.loc[:, \"m\"] = results[\"data\"].str.extract(\".*m-(.*?)_.*\").values.flatten().astype(int)\n",
    "results.loc[:, \"n\"] = results[\"data\"].str.extract(\".*n-(.*?)_\").values.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "ax = axes\n",
    "\n",
    "ax.axvline(20000, linestyle=\"-\", color=\"lightgrey\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"python\") & (results[\"algorithm\"] == \"topk_matrix\") & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[0], label=\"Python cor\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"python\") & (results[\"algorithm\"] == \"topk_balltree_combined_tree\") & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[0], linestyle=\"--\", label=\"Python LR\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"julia\") & (results[\"algorithm\"] == \"topk_matrix\") & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[1], label=\"Julia cor\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"julia\") & (results[\"algorithm\"] == \"topk_balltree_nn_combined_tree\") & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[1], linestyle=\"--\", label=\"Julia LR\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"r\") & (results[\"algorithm\"] == \"topk_matrix\") & (results[\"m\"] == 50)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"n\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[2], label=\"R cor\", marker=\"+\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "# ax.set_xlim([1000, 20000])\n",
    "\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "ax.set(xlabel=\"features\", ylabel=\"seconds\")\n",
    "\n",
    "ax.set_xticks([1000,3000,10000,30000])\n",
    "ax.set_xticklabels([1000,3000,10000,30000])\n",
    "\n",
    "ax.set_yticks([0.1, 1, 10,100, 1000])\n",
    "ax.set_yticklabels([0.1,1,10,100, 1000])\n",
    "\n",
    "fig.savefig(out_dir / \"topk_runtime_features.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "ax = axes\n",
    "\n",
    "ax.axvline(50, linestyle=\"-\", color=\"lightgrey\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"python\") & (results[\"algorithm\"] == \"topk_matrix\") & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[0], label=\"Python cor\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"python\") & (results[\"algorithm\"] == \"topk_balltree_combined_tree\") & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[0], linestyle=\"--\", label=\"Python LR\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"julia\") & (results[\"algorithm\"] == \"topk_matrix\") & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[1], label=\"Julia cor\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"julia\") & (results[\"algorithm\"] == \"topk_balltree_nn_combined_tree\") & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[1], linestyle=\"--\", label=\"Julia LR\", marker=\"+\")\n",
    "\n",
    "select = results[(results[\"language\"] == \"r\") & (results[\"algorithm\"] == \"topk_matrix\") & (results[\"n\"] == 20000)][[\"m\", \"n\", \"runtime\", \"memory\"]].groupby(\"m\").median()\n",
    "ax.plot(select.index, select[\"runtime\"], color=colors[2], label=\"R cor\", marker=\"+\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "# plt.xlim([0,100])\n",
    "\n",
    "ax.set_xticks([10,25, 50,100, 200,500])\n",
    "ax.set_xticklabels([10,25,50, 100, 200,500])\n",
    "\n",
    "\n",
    "ax.set_yticks([1,10,100, 1000])\n",
    "ax.set_yticklabels([1,10,100, 1000])\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "    \n",
    "ax.set(xlabel=\"samples\", ylabel=\"seconds\")\n",
    "fig.savefig(out_dir / \"topk_runtime_samples.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "ax = axes\n",
    "\n",
    "r = results[(results[\"n\"] == 20000) & (results[\"m\"] == 50)]\n",
    "sns.barplot(\n",
    "    x=[n.capitalize() for n in r[\"language\"].values], \n",
    "    y=r[\"memory\"].values / 1024, \n",
    "    hue=[f\"{method_name}\" if \"balltree\" in n else \"default\" for n in r[\"algorithm\"].values], \n",
    "    ax=ax, \n",
    "    palette=[\"black\", \"grey\"])\n",
    "# sns.swarmplot(\n",
    "#     x=[n.capitalize() for n in r[\"language\"].values], \n",
    "#     y=r[\"memory\"].values / 1024, \n",
    "#     hue=[f\"{method_name}\" if \"balltree\" in n else \"default\" for n in r[\"algorithm\"].values], \n",
    "#     ax=ax, \n",
    "#     palette=[\"black\", \"grey\"])\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_ylim([0.1,11])\n",
    "\n",
    "# plt.yscale(\"log\")\n",
    "ax.set(xlabel=None, ylabel=\"memory (Gb)\")\n",
    "\n",
    "fig.savefig(out_dir / \"topk_memory_50x20000.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = benchmark_experiments.copy()\n",
    "results[\"algorithm_base\"] = results.algorithm.str.replace(\"(_parallel.*)?(_nthreads.*)?\", \"\")\n",
    "results[\"algorithm_parallel\"] = results.algorithm.str.extract(\".*_parallel_([0-9]+).*\")\n",
    "results[\"algorithm_threads\"] = results.algorithm.str.extract(\".*_nthreads-([0-9]+).*\")\n",
    "results.loc[results.algorithm_parallel.isna(),\"algorithm_parallel\"] = 1 \n",
    "results.loc[results.algorithm_threads.isna(),\"algorithm_threads\"] = 1 \n",
    "\n",
    "results[\"algorithm_parallel\"] = results[\"algorithm_parallel\"].astype(int)\n",
    "results[\"algorithm_threads\"] = results[\"algorithm_threads\"].astype(int)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.data.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results[\n",
    "    (results.context == \"fast\") \n",
    "    & (results.language == \"python\") \n",
    "    & (results.data.str.contains(\"cancer_postprocessed_nonegatives_dropduplicates_sample-0.25\"))\n",
    "]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using copy actually increases runtimes with more cores!\n",
    "algorithms_python = [\"cor_corrcoef\", \"cor_matrix_symmetrical\", \"cor_matrix_symmetrical_nocopy\"]\n",
    "# algorithms_python = [\"cor_corrcoef\", \"cor_matrix_symmetrical_nocopy\"]\n",
    "# algorithms_python = [\"cor_corrcoef\", \"cor_matrix_symmetrical\"]\n",
    "# algorithms_python = [\"cor_matrix_symmetrical\"]\n",
    "# algorithms_python = [\"cor_matrix_symmetrical_nocopy\"]\n",
    "# algorithms_python = [\"cor_matrix_symmetrical\", \"cor_matrix_symmetrical_nocopy\"]\n",
    "algorithms_julia = []\n",
    "\n",
    "r = results[\n",
    "    results.algorithm_base.isin(algorithms_python) \n",
    "    & (results.context == \"fast\") \n",
    "    & (results.language == \"python\") \n",
    "    & (results.data.str.contains(\"cancer_postprocessed_nonnegatives_dropduplicates_sample-0.25\"))]\n",
    "\n",
    "r = results[\n",
    "#     results.algorithm_base.isin(algorithms_python) \n",
    "    (results.context == \"fast\") \n",
    "    & (results.language == \"python\") \n",
    "    & (results.data.str.contains(\"cancer_postprocessed_nonegatives_dropduplicates_sample-0.25\"))\n",
    "]\n",
    "r\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "sns.lineplot(x=r.algorithm_threads.values, y=r.runtime.values, hue=r.algorithm_base.values, ax=ax)#, order=[1,2,4,8,16,32,64])\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "\n",
    "ax.set_xticks([1,2,4,8,16,32,64])\n",
    "ax.set_xticklabels([1,2,4,8,16,32,64])\n",
    "# plt.yscale(\"log\")\n",
    "# plt.ylim([5,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rr = r[(r.algorithm_base == \"cor_matrix_symmetrical_nocopy\")].groupby([\"algorithm_base\", \"algorithm_threads\"]).median().reset_index()\n",
    "rr2 = r[(r.algorithm_base == \"cor_matrix_symmetrical\")].groupby([\"algorithm_base\", \"algorithm_threads\"]).median().reset_index()\n",
    "rr3 = r[(r.algorithm_base == \"cor_corrcoef\")].groupby([\"algorithm_base\", \"algorithm_threads\"]).median().reset_index()\n",
    "\n",
    "order = np.argsort(rr[\"algorithm_threads\"].values)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "\n",
    "g1, = ax1.plot(\n",
    "    rr2[\"algorithm_threads\"].values[order], \n",
    "    rr2[\"runtime\"].values[order],\n",
    "    marker=\"o\",\n",
    "    color=colors[0],\n",
    "    linestyle=\"--\")\n",
    "\n",
    "g1, = ax1.plot(\n",
    "    rr[\"algorithm_threads\"].values[order], \n",
    "    rr[\"runtime\"].values[order],\n",
    "    marker=\"o\",\n",
    "    color=colors[0],\n",
    "    linestyle=\":\")\n",
    "\n",
    "\n",
    "g1, = ax1.plot(\n",
    "    rr3[\"algorithm_threads\"].values[order], \n",
    "    rr3[\"runtime\"].values[order],\n",
    "    marker=\"o\",\n",
    "    color=colors[0],\n",
    "    linestyle=\"-\")\n",
    "\n",
    "# ax1.scatter(\n",
    "#     [64], \n",
    "#     [6],\n",
    "#     marker=\"o\",\n",
    "#     color=\"green\")\n",
    "\n",
    "ax1.set_xticks(rr[\"algorithm_threads\"].values)\n",
    "ax1.set_ylabel(\"seconds\", color=g1.get_color())\n",
    "ax1.set_xlabel(\"cores\")\n",
    "\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    rr[\"algorithm_threads\"].values[order], \n",
    "    rr[\"memory\"].values[order] / 1000,\n",
    "    color=\"lightgrey\", \n",
    "    width=.3 * rr[\"algorithm_threads\"].values[order],\n",
    "    align=\"center\")\n",
    "\n",
    "ax2.set_ylabel(\"memory (Gb)\", color=\"grey\")\n",
    "ax2.set_ylim([0,66])\n",
    "\n",
    "ax2.spines['top'].set_visible(False)\n",
    "\n",
    "ax1.set_zorder(1)\n",
    "ax1.patch.set_visible(False)\n",
    "\n",
    "ax1.set(xscale=\"log\", yscale=\"log\")\n",
    "ax2.set(xscale=\"log\")\n",
    "ax1.set_xticks(rr[\"algorithm_threads\"].values)\n",
    "ax1.set_xticklabels(rr[\"algorithm_threads\"].values)\n",
    "\n",
    "ax1.set_yticks([5, 10,20,40, 80,160])\n",
    "ax1.set_yticklabels([5, 10,20,40,80,160])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\n",
    "    f\"../_out/figures/threads_real-data_fast.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "patchList = []\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[0], linestyle=\"-\", \n",
    "    label=\"corrcoef\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[0], linestyle=\"--\", \n",
    "    label=f\"{method_name} (copy)\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "\n",
    "data_key = Line2D(\n",
    "    [0], [0], marker=\"o\", color=colors[0], linestyle=\":\", \n",
    "    label=f\"{method_name} (no copy)\", \n",
    "    markersize=0)\n",
    "patchList.append(data_key)\n",
    "\n",
    "fig, axes = plt.subplots(1,1, dpi=150, figsize=(2,2))\n",
    "ax = axes\n",
    "ax.legend(handles=patchList, loc=\"upper left\", frameon=False)\n",
    "ax.axis(\"off\")\n",
    "fig.savefig('../_out/figures/legend_parallel.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.algorithm_parallel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results[\n",
    "#     results.algorithm_base.isin(algorithms_python) \n",
    "#     &\n",
    "    (results.context == \"topk\") \n",
    "    & (results.language == \"python\") \n",
    "    & (results.context_param == 0.1) \n",
    "    & (results.data.str.contains(\"cancer_postprocessed_nonegatives_dropduplicates_sample-1.00\"))\n",
    "]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = r[r.algorithm_base.str.startswith(\"topk_balltree_combined_tree_optimized\") & (r.prefix == \"topk_default\") & (r.memory_backend == \"psutil\")]\\\n",
    "    .groupby([\"algorithm_base\", \"algorithm_parallel\"])\\\n",
    "    .agg([\"mean\", \"count\"])\\\n",
    "    .reset_index()\n",
    "display(runtimes)\n",
    "\n",
    "memory = r[r.algorithm_base.str.startswith(\"topk_balltree_combined_tree_optimized\") & (r.prefix == \"topk_default\") & (r.memory_backend == \"psutil_uss\")]\\\n",
    "    .groupby([\"algorithm_base\", \"algorithm_parallel\"])\\\n",
    "    .agg([\"mean\", \"count\"])\\\n",
    "    .reset_index()\n",
    "display(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = runtimes.copy()\n",
    "aggregate.loc[aggregate.algorithm_base.str.startswith(\"topk_balltree_combined_tree_optimized\").values, (\"memory\", \"mean\")] = memory.memory[\"mean\"].values\n",
    "aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rr = aggregate\n",
    "\n",
    "order = np.argsort(rr[\"algorithm_parallel\"].values)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "\n",
    "g1, = ax1.plot(\n",
    "    rr[\"algorithm_parallel\"].values[order], \n",
    "    rr[\"runtime\"][\"mean\"].values[order] / 60,\n",
    "    marker=\"o\")\n",
    "\n",
    "ax1.set_xticks(rr[\"algorithm_parallel\"].values)\n",
    "ax1.set_ylabel(\"minutes\", color=g1.get_color())\n",
    "ax1.set_xlabel(\"cores\")\n",
    "\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(\n",
    "    rr[\"algorithm_parallel\"].values[order], \n",
    "    rr[\"memory\"][\"mean\"].values[order] / 1000,\n",
    "    color=\"lightgrey\", \n",
    "    width=.3 * rr[\"algorithm_parallel\"].values[order],\n",
    "    align=\"center\")\n",
    "\n",
    "ax2.set_ylabel(\"memory (Gb)\", color=\"grey\")\n",
    "ax2.set_ylim([0,80])\n",
    "\n",
    "ax2.spines['top'].set_visible(False)\n",
    "\n",
    "ax1.set_zorder(1)\n",
    "ax1.patch.set_visible(False)\n",
    "\n",
    "# ax1.set(xscale=\"log\", yscale=\"log\")\n",
    "ax2.set(xscale=\"log\")\n",
    "\n",
    "ax1.set_xticks(rr[\"algorithm_parallel\"].values)\n",
    "ax1.set_xticklabels(rr[\"algorithm_parallel\"].values)\n",
    "\n",
    "# ax1.set_yticks(rr[\"algorithm_parallel\"].values)\n",
    "# ax1.set_yticklabels(rr[\"algorithm_parallel\"].values)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(out_dir / \"threads_real-data_topk.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive comparison with other libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also see `scripts` for small scale experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracies = [\n",
    "    (\"preeclampsia\", accuracy_experiments.loc[(\"preeclampsia_negative_dropduplicates\", \"topk\", 1, \"unknown\")]),\n",
    "    (\"pregnancy\", accuracy_experiments.loc[(\"pregnancy_negative_dropduplicates\", \"topk\", 1, \"unknown\")]),\n",
    "    (\"cancer\", accuracy_experiments.loc[(\"cancer_negative_dropduplicates_sample-0.25\", \"topk\", 1, \"unknown\")])\n",
    "]\n",
    "\n",
    "\n",
    "for c in accuracies[0][1].columns:\n",
    "    \n",
    "    fig, axes = plt.subplots(1,1, figsize=(3,2), dpi=150)\n",
    "    ax = axes\n",
    "        \n",
    "    for n, accuracy in accuracies: \n",
    "#         display(accuracy)\n",
    "        ax.plot(accuracy.reset_index()[\"approximation_factor\"], accuracy[c].values, label=n, marker=\"+\")\n",
    "        \n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    ax.set(xlabel=\"approximation factor\", ylabel=c)\n",
    "    ax.legend()\n",
    "    fig.savefig(\n",
    "        f\"../_out/figures/accuracy_real-data_topk-1percent_{c}.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0dee6959e37668cfcbb476153b942740b6917fd46088aaeb4273eaf2a18c4d3a"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
